"""
æ€§èƒ½åŸºå‡†æµ‹è¯•è„šæœ¬ â€”â€” çœŸå®æµ‹é‡èµ„æºå ç”¨ä¸æ¨ç†å»¶è¿Ÿ

åŠŸèƒ½ï¼š
1. åŠ è½½æ¨¡å‹å¹¶è®°å½•åŠ è½½æ—¶é—´ã€æ˜¾å­˜å ç”¨ã€ç³»ç»Ÿå†…å­˜
2. å¯¹å¤šç§åœºæ™¯ï¼ˆçº¯æ–‡æœ¬ã€å›¾ç‰‡ç†è§£ã€PDFè§£æã€è§†é¢‘ç†è§£ï¼‰è¿›è¡Œæ¨ç†æµ‹è¯•
3. è®°å½•æ¯ä¸ªåœºæ™¯çš„é¦– token å»¶è¿Ÿã€æ€»å»¶è¿Ÿã€ç”Ÿæˆ token æ•°ã€ååé‡
4. æ”¶é›†è¾“å…¥è¾“å‡ºç¤ºä¾‹
5. å°†æ‰€æœ‰æ•°æ®ä¿å­˜ä¸º JSONï¼Œä¾›æ–‡æ¡£ç”Ÿæˆè„šæœ¬ä½¿ç”¨

Usage:
    # Transformers åç«¯
    python scripts/benchmark.py --backend transformers

    # vLLM åç«¯
    conda activate vllm
    python scripts/benchmark.py --backend vllm
"""

import argparse
import json
import os
import sys
import time
import platform
import tempfile
from datetime import datetime

import torch
import psutil
from PIL import Image, ImageDraw, ImageFont

# å°†é¡¹ç›®æ ¹ç›®å½•åŠ å…¥ path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from my_doubao_app import get_gpu_memory_nvidia_smi


# â”€â”€â”€ å·¥å…·å‡½æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_gpu_info() -> dict:
    """è·å– GPU ä¿¡æ¯å’Œå½“å‰æ˜¾å­˜ä½¿ç”¨æƒ…å†µã€‚"""
    if not torch.cuda.is_available():
        return {"available": False}
    info = {
        "available": True,
        "device_name": torch.cuda.get_device_name(0),
        "device_count": torch.cuda.device_count(),
        "total_memory_gb": round(torch.cuda.get_device_properties(0).total_memory / 1024**3, 2),
        "allocated_gb": round(torch.cuda.memory_allocated(0) / 1024**3, 2),
        "reserved_gb": round(torch.cuda.memory_reserved(0) / 1024**3, 2),
        "max_allocated_gb": round(torch.cuda.max_memory_allocated(0) / 1024**3, 2),
    }
    # å°è¯•è·å– nvidia-smi ä¿¡æ¯
    try:
        import subprocess
        result = subprocess.run(
            ["nvidia-smi", "--query-gpu=memory.used,memory.total,utilization.gpu",
             "--format=csv,noheader,nounits"],
            capture_output=True, text=True, timeout=5
        )
        if result.returncode == 0:
            parts = result.stdout.strip().split(",")
            info["nvidia_smi_used_mb"] = int(parts[0].strip())
            info["nvidia_smi_total_mb"] = int(parts[1].strip())
            info["nvidia_smi_gpu_util_pct"] = int(parts[2].strip())
    except Exception:
        pass
    return info


def get_system_info() -> dict:
    """è·å–ç³»ç»Ÿä¿¡æ¯ã€‚"""
    mem = psutil.virtual_memory()
    return {
        "platform": platform.platform(),
        "python_version": platform.python_version(),
        "cpu_count": psutil.cpu_count(logical=True),
        "total_ram_gb": round(mem.total / 1024**3, 2),
        "available_ram_gb": round(mem.available / 1024**3, 2),
        "used_ram_gb": round(mem.used / 1024**3, 2),
        "ram_percent": mem.percent,
    }


def get_torch_info() -> dict:
    """è·å– PyTorch / CUDA ç‰ˆæœ¬ä¿¡æ¯ã€‚"""
    info = {
        "torch_version": torch.__version__,
        "cuda_available": torch.cuda.is_available(),
    }
    if torch.cuda.is_available():
        info["cuda_version"] = torch.version.cuda
        info["cudnn_version"] = str(torch.backends.cudnn.version())
    return info


def create_test_image(width=640, height=480, text="Test Image") -> str:
    """åˆ›å»ºä¸€å¼ æµ‹è¯•å›¾ç‰‡å¹¶è¿”å›ä¸´æ—¶æ–‡ä»¶è·¯å¾„ã€‚"""
    img = Image.new("RGB", (width, height), color=(135, 206, 235))
    draw = ImageDraw.Draw(img)
    # ç”»ä¸€äº›ç®€å•å›¾å½¢
    draw.rectangle([50, 50, 200, 200], fill=(255, 100, 100))
    draw.ellipse([300, 100, 500, 300], fill=(100, 255, 100))
    draw.rectangle([150, 250, 450, 400], fill=(100, 100, 255))
    # æ·»åŠ æ–‡å­—
    try:
        draw.text((width // 2 - 40, 20), text, fill=(0, 0, 0))
    except Exception:
        pass
    tmp = tempfile.NamedTemporaryFile(suffix=".jpg", delete=False)
    img.save(tmp.name)
    tmp.close()
    return tmp.name


def create_test_pdf(pages=3) -> str:
    """åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯• PDF æ–‡ä»¶ã€‚"""
    try:
        import fitz as pymupdf
    except ImportError:
        return None

    doc = pymupdf.open()
    for i in range(pages):
        page = doc.new_page(width=595, height=842)  # A4
        text = f"è¿™æ˜¯æµ‹è¯• PDF çš„ç¬¬ {i + 1} é¡µ\n\n" \
               f"æœ¬é¡µåŒ…å«ä¸€äº›ç¤ºä¾‹æ–‡æœ¬ï¼Œç”¨äºæµ‹è¯•æ¨¡å‹çš„ PDF æ–‡æ¡£ç†è§£èƒ½åŠ›ã€‚\n\n" \
               f"äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„ç³»ç»Ÿã€‚\n" \
               f"æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ•°æ®çš„è¡¨ç¤ºã€‚"
        page.insert_text((72, 72), text, fontsize=14, fontname="china-s")
    tmp = tempfile.NamedTemporaryFile(suffix=".pdf", delete=False)
    doc.save(tmp.name)
    doc.close()
    tmp.close()
    return tmp.name


def count_tokens(processor, text: str) -> int:
    """ä¼°ç®—æ–‡æœ¬çš„ token æ•°ã€‚"""
    try:
        return len(processor.tokenizer.encode(text))
    except Exception:
        return len(text) // 2  # ç²—ç•¥ä¼°ç®—


# â”€â”€â”€ åŸºå‡†æµ‹è¯•ç±» â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class BenchmarkRunner:
    def __init__(self, model_path: str, backend: str, max_new_tokens: int = 512,
                 gpu_memory_utilization: float = 0.9, max_model_len: int = 8192):
        self.model_path = model_path
        self.backend = backend
        self.max_new_tokens = max_new_tokens
        self.results = {
            "metadata": {
                "model_path": model_path,
                "backend": backend,
                "max_new_tokens": max_new_tokens,
                "timestamp": datetime.now().isoformat(),
            },
            "system_info": get_system_info(),
            "torch_info": get_torch_info(),
            "gpu_info_before_load": get_gpu_info(),
            "model_loading": {},
            "gpu_info_after_load": {},
            "benchmarks": [],
            "resource_summary": {},
        }

        # åŠ è½½æ¨¡å‹
        print("=" * 60)
        print(f"[åŸºå‡†æµ‹è¯•] åç«¯: {backend}, æ¨¡å‹: {model_path}")
        print("=" * 60)

        if torch.cuda.is_available() and backend != "vllm":
            torch.cuda.reset_peak_memory_stats()

        mem_before = psutil.Process().memory_info().rss / 1024**3
        smi_before = get_gpu_memory_nvidia_smi()
        gpu_before = torch.cuda.memory_allocated(0) / 1024**3 if (torch.cuda.is_available() and backend != "vllm") else smi_before["used_gb"]

        t0 = time.time()
        if backend == "vllm":
            from my_doubao_app import VllmDoubaoAssistant
            self.assistant = VllmDoubaoAssistant(
                model_path=model_path,
                max_new_tokens=max_new_tokens,
                gpu_memory_utilization=gpu_memory_utilization,
                max_model_len=max_model_len,
            )
            self.processor = None
        else:
            from my_doubao_app import DoubaoAssistant
            self.assistant = DoubaoAssistant(
                model_path=model_path,
                max_new_tokens=max_new_tokens,
            )
            self.processor = self.assistant.processor
        load_time = time.time() - t0

        mem_after = psutil.Process().memory_info().rss / 1024**3
        smi_after = get_gpu_memory_nvidia_smi()
        if backend == "vllm":
            gpu_after = smi_after["used_gb"]
        else:
            gpu_after = torch.cuda.memory_allocated(0) / 1024**3 if torch.cuda.is_available() else 0

        self.results["model_loading"] = {
            "load_time_seconds": round(load_time, 2),
            "system_memory_before_gb": round(mem_before, 2),
            "system_memory_after_gb": round(mem_after, 2),
            "system_memory_delta_gb": round(mem_after - mem_before, 2),
            "gpu_memory_before_gb": round(gpu_before, 2),
            "gpu_memory_after_gb": round(gpu_after, 2),
            "gpu_memory_delta_gb": round(gpu_after - gpu_before, 2),
        }
        self.results["gpu_info_after_load"] = get_gpu_info()

        print(f"\nâœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œè€—æ—¶ {load_time:.2f} ç§’")
        print(f"   GPU æ˜¾å­˜: {gpu_before:.2f} GB â†’ {gpu_after:.2f} GB (å¢åŠ  {gpu_after - gpu_before:.2f} GB)")
        print(f"   ç³»ç»Ÿå†…å­˜: {mem_before:.2f} GB â†’ {mem_after:.2f} GB (å¢åŠ  {mem_after - mem_before:.2f} GB)")

    def _run_single_benchmark(self, name: str, user_text: str,
                               media_path: str = None,
                               extra_images: list = None,
                               warmup: bool = False) -> dict:
        """è¿è¡Œå•ä¸ªåŸºå‡†æµ‹è¯•åœºæ™¯ã€‚"""
        print(f"\n{'â”€' * 50}")
        print(f"{'[é¢„çƒ­]' if warmup else '[æµ‹è¯•]'} {name}")
        print(f"  è¾“å…¥: {user_text[:80]}{'...' if len(user_text) > 80 else ''}")
        if media_path:
            print(f"  åª’ä½“: {media_path}")
        if extra_images:
            print(f"  é¢å¤–å›¾ç‰‡: {len(extra_images)} å¼ ")

        # æ¸…ç† GPU ç¼“å­˜
        is_vllm = self.backend == "vllm"
        if torch.cuda.is_available() and not is_vllm:
            torch.cuda.empty_cache()
            torch.cuda.reset_peak_memory_stats()

        # nvidia-smi åŸºçº¿ï¼ˆå¯¹ vLLM æœ‰æ•ˆï¼‰
        smi_before = get_gpu_memory_nvidia_smi()
        smi_peak = smi_before["used_gb"]

        gpu_before = torch.cuda.memory_allocated(0) / 1024**3 if (torch.cuda.is_available() and not is_vllm) else smi_before["used_gb"]
        mem_before = psutil.Process().memory_info().rss / 1024**3

        conversation = []
        answer = ""
        first_token_time = None
        token_count = 0

        t_start = time.time()

        try:
            token_stream, updated_conversation = self.assistant.chat_stream(
                conversation, user_text, media_path,
                temperature=0.7, top_p=0.9,
                extra_images=extra_images,
            )

            for token in token_stream:
                if token.startswith("\r"):
                    continue
                if first_token_time is None:
                    first_token_time = time.time()
                answer += token
                token_count += 1
                # vLLM: å®šæœŸé‡‡æ · nvidia-smi å³°å€¼
                if is_vllm and token_count % 10 == 0:
                    cur_smi = get_gpu_memory_nvidia_smi()
                    smi_peak = max(smi_peak, cur_smi["used_gb"])

        except Exception as exc:
            answer = f"[é”™è¯¯] {exc}"
            print(f"  âŒ é”™è¯¯: {exc}")

        t_end = time.time()

        total_time = t_end - t_start
        ttft = (first_token_time - t_start) if first_token_time else total_time

        # æœ€ç»ˆæ˜¾å­˜é‡‡æ ·
        smi_after = get_gpu_memory_nvidia_smi()
        smi_peak = max(smi_peak, smi_after["used_gb"])

        if is_vllm:
            gpu_after_peak = smi_peak
            gpu_after = smi_after["used_gb"]
        else:
            gpu_after_peak = torch.cuda.max_memory_allocated(0) / 1024**3 if torch.cuda.is_available() else 0
            gpu_after = torch.cuda.memory_allocated(0) / 1024**3 if torch.cuda.is_available() else 0
        mem_after = psutil.Process().memory_info().rss / 1024**3

        # ä¼°ç®—è¾“å‡º token æ•°
        output_tokens = 0
        if self.processor:
            output_tokens = count_tokens(self.processor, answer.strip())
        else:
            output_tokens = len(answer.strip()) // 2  # ç²—ç•¥ä¼°ç®—

        tokens_per_sec = output_tokens / total_time if total_time > 0 else 0

        result = {
            "name": name,
            "input_text": user_text,
            "has_media": media_path is not None,
            "has_extra_images": bool(extra_images),
            "output_text": answer.strip(),
            "output_tokens_est": output_tokens,
            "timing": {
                "total_seconds": round(total_time, 3),
                "time_to_first_token_seconds": round(ttft, 3),
                "tokens_per_second": round(tokens_per_sec, 2),
            },
            "memory": {
                "gpu_before_gb": round(gpu_before, 2),
                "gpu_peak_gb": round(gpu_after_peak, 2),
                "gpu_after_gb": round(gpu_after, 2),
                "gpu_inference_delta_gb": round(gpu_after_peak - gpu_before, 2),
                "system_memory_before_gb": round(mem_before, 2),
                "system_memory_after_gb": round(mem_after, 2),
            },
        }

        print(f"  âœ… å®Œæˆ: {total_time:.2f}s (é¦–token {ttft:.2f}s), "
              f"~{output_tokens} tokens, {tokens_per_sec:.1f} tok/s")
        print(f"  GPU å³°å€¼: {gpu_after_peak:.2f} GB, æ¨ç†å¢é‡: {gpu_after_peak - gpu_before:.2f} GB")
        print(f"  å›å¤: {answer.strip()[:120]}{'...' if len(answer.strip()) > 120 else ''}")

        return result

    def run_all_benchmarks(self):
        """è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•åœºæ™¯ã€‚"""

        # â”€â”€ é¢„çƒ­ â”€â”€
        print("\n" + "=" * 60)
        print("[é˜¶æ®µ 1/2] é¢„çƒ­æ¨ç†å¼•æ“...")
        print("=" * 60)
        self._run_single_benchmark(
            "é¢„çƒ­", "ä½ å¥½", warmup=True
        )

        # â”€â”€ æ­£å¼æµ‹è¯• â”€â”€
        print("\n" + "=" * 60)
        print("[é˜¶æ®µ 2/2] æ­£å¼åŸºå‡†æµ‹è¯•")
        print("=" * 60)

        # åœºæ™¯ 1: çº¯æ–‡æœ¬çŸ­é—®ç­”
        r1 = self._run_single_benchmark(
            "çº¯æ–‡æœ¬çŸ­é—®ç­”",
            "ä½ å¥½ï¼Œè¯·ç”¨ä¸¤ä¸‰å¥è¯ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"
        )
        self.results["benchmarks"].append(r1)

        # åœºæ™¯ 2: çº¯æ–‡æœ¬é•¿å›ç­”
        r2 = self._run_single_benchmark(
            "çº¯æ–‡æœ¬é•¿å›ç­”",
            "è¯·è¯¦ç»†è§£é‡Šä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼ŒåŒ…æ‹¬å…¶åŸºæœ¬åŸç†ã€ä¸»è¦æ¶æ„ï¼ˆå¦‚CNNã€RNNã€Transformerï¼‰ã€"
            "è®­ç»ƒè¿‡ç¨‹ä»¥åŠåœ¨è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„åº”ç”¨ã€‚"
        )
        self.results["benchmarks"].append(r2)

        # åœºæ™¯ 3: å›¾ç‰‡ç†è§£
        test_img = create_test_image(640, 480, "Benchmark Test")
        r3 = self._run_single_benchmark(
            "å›¾ç‰‡ç†è§£",
            "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡ä¸­çš„å†…å®¹ï¼ŒåŒ…æ‹¬é¢œè‰²ã€å½¢çŠ¶å’Œå¸ƒå±€ã€‚",
            media_path=test_img,
        )
        self.results["benchmarks"].append(r3)
        os.unlink(test_img)

        # åœºæ™¯ 4: PDF æ–‡æ¡£è§£æ
        test_pdf = create_test_pdf(3)
        if test_pdf:
            try:
                from my_doubao_app import pdf_to_images
                pdf_pages = pdf_to_images(test_pdf, max_pages=3)
                r4 = self._run_single_benchmark(
                    "PDFæ–‡æ¡£è§£æ(3é¡µ)",
                    "è¯·æ€»ç»“è¿™ä»½æ–‡æ¡£çš„ä¸»è¦å†…å®¹ã€‚",
                    extra_images=pdf_pages,
                )
                self.results["benchmarks"].append(r4)
                for p in pdf_pages:
                    os.unlink(p)
                os.unlink(test_pdf)
            except Exception as exc:
                print(f"  âš ï¸ PDF æµ‹è¯•è·³è¿‡: {exc}")
        else:
            print("  âš ï¸ PDF æµ‹è¯•è·³è¿‡: PyMuPDF æœªå®‰è£…")

        # åœºæ™¯ 5: å¤šè½®å¯¹è¯ï¼ˆæ¨¡æ‹Ÿä¸¤è½®ï¼‰
        r5 = self._run_single_benchmark(
            "å¤šè½®å¯¹è¯-ç¬¬1è½®",
            "ä¸­å›½çš„é¦–éƒ½æ˜¯å“ªé‡Œï¼Ÿ"
        )
        self.results["benchmarks"].append(r5)

        # åœºæ™¯ 6: æ•°å­¦/æ¨ç†
        r6 = self._run_single_benchmark(
            "æ•°å­¦æ¨ç†",
            "ä¸€ä¸ªæ°´æ± æœ‰ä¸¤ä¸ªè¿›æ°´ç®¡å’Œä¸€ä¸ªå‡ºæ°´ç®¡ã€‚è¿›æ°´ç®¡Aæ¯å°æ—¶æ³¨å…¥3å¨æ°´ï¼Œè¿›æ°´ç®¡Bæ¯å°æ—¶æ³¨å…¥2å¨æ°´ï¼Œ"
            "å‡ºæ°´ç®¡æ¯å°æ—¶æ’å‡º1å¨æ°´ã€‚å¦‚æœæ°´æ± å®¹é‡ä¸º40å¨ï¼Œä»ç©ºæ± å¼€å§‹ï¼Œéœ€è¦å¤šå°‘å°æ—¶æ‰èƒ½æ³¨æ»¡ï¼Ÿè¯·åˆ—å‡ºè®¡ç®—è¿‡ç¨‹ã€‚"
        )
        self.results["benchmarks"].append(r6)

        # â”€â”€ æ±‡æ€» â”€â”€
        self._compute_summary()

    def _compute_summary(self):
        """è®¡ç®—èµ„æºå ç”¨æ±‡æ€»ã€‚"""
        benchmarks = self.results["benchmarks"]
        if not benchmarks:
            return

        gpu_peaks = [b["memory"]["gpu_peak_gb"] for b in benchmarks]
        total_times = [b["timing"]["total_seconds"] for b in benchmarks]
        ttfts = [b["timing"]["time_to_first_token_seconds"] for b in benchmarks]
        tps_list = [b["timing"]["tokens_per_second"] for b in benchmarks if b["timing"]["tokens_per_second"] > 0]

        self.results["resource_summary"] = {
            "model_load_gpu_gb": self.results["model_loading"]["gpu_memory_delta_gb"],
            "model_load_time_seconds": self.results["model_loading"]["load_time_seconds"],
            "model_load_system_memory_gb": self.results["model_loading"]["system_memory_delta_gb"],
            "inference_gpu_peak_max_gb": round(max(gpu_peaks), 2),
            "inference_gpu_peak_avg_gb": round(sum(gpu_peaks) / len(gpu_peaks), 2),
            "avg_total_time_seconds": round(sum(total_times) / len(total_times), 2),
            "avg_ttft_seconds": round(sum(ttfts) / len(ttfts), 2),
            "avg_tokens_per_second": round(sum(tps_list) / len(tps_list), 2) if tps_list else 0,
            "gpu_total_memory_gb": self.results["gpu_info_after_load"].get("total_memory_gb", "N/A"),
        }

        print("\n" + "=" * 60)
        print("[æ±‡æ€»] èµ„æºå ç”¨ä¸æ€§èƒ½")
        print("=" * 60)
        s = self.results["resource_summary"]
        print(f"  æ¨¡å‹åŠ è½½æ˜¾å­˜:     {s['model_load_gpu_gb']:.2f} GB")
        print(f"  æ¨¡å‹åŠ è½½æ—¶é—´:     {s['model_load_time_seconds']:.2f} ç§’")
        print(f"  æ¨ç†å³°å€¼æ˜¾å­˜(æœ€å¤§): {s['inference_gpu_peak_max_gb']:.2f} GB")
        print(f"  æ¨ç†å³°å€¼æ˜¾å­˜(å¹³å‡): {s['inference_gpu_peak_avg_gb']:.2f} GB")
        print(f"  å¹³å‡æ€»å»¶è¿Ÿ:       {s['avg_total_time_seconds']:.2f} ç§’")
        print(f"  å¹³å‡é¦–tokenå»¶è¿Ÿ:  {s['avg_ttft_seconds']:.2f} ç§’")
        print(f"  å¹³å‡ååé‡:       {s['avg_tokens_per_second']:.1f} tokens/s")

    def save_results(self, output_path: str):
        """ä¿å­˜ç»“æœåˆ° JSON æ–‡ä»¶ã€‚"""
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ“„ åŸºå‡†æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {output_path}")


def parse_args():
    parser = argparse.ArgumentParser(description="Qwen3-VL æ€§èƒ½åŸºå‡†æµ‹è¯•")
    parser.add_argument("--model-path", type=str, default="./Qwen3-VL-2B-Instruct")
    parser.add_argument("--backend", type=str, default="transformers",
                        choices=["transformers", "vllm"])
    parser.add_argument("--max-new-tokens", type=int, default=512)
    parser.add_argument("--gpu-memory-utilization", type=float, default=0.9)
    parser.add_argument("--max-model-len", type=int, default=8192)
    parser.add_argument("--output", type=str, default=None,
                        help="è¾“å‡º JSON æ–‡ä»¶è·¯å¾„ (é»˜è®¤: benchmark_results_{backend}.json)")
    return parser.parse_args()


def main():
    args = parse_args()
    output_path = args.output or f"benchmark_results_{args.backend}.json"

    runner = BenchmarkRunner(
        model_path=args.model_path,
        backend=args.backend,
        max_new_tokens=args.max_new_tokens,
        gpu_memory_utilization=args.gpu_memory_utilization,
        max_model_len=args.max_model_len,
    )
    runner.run_all_benchmarks()
    runner.save_results(output_path)

    print("\nâœ… åŸºå‡†æµ‹è¯•å®Œæˆï¼")
    print(f"   ç»“æœæ–‡ä»¶: {output_path}")
    print(f"   æ¥ä¸‹æ¥è¿è¡Œ: python scripts/generate_report.py --input {output_path}")


if __name__ == "__main__":
    main()
